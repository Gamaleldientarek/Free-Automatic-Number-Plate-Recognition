{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1cAxdvd61e"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
        "\n",
        "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/notebooks/blob/main/notebooks/how-to-use-ultralytics-yolo-with-openai-for-number-plate-recognition.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  \n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
        "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
        "  \n",
        "  Welcome to the ANPR with Ultralytics YOLO11 notebook! <a href=\"https://github.com/ultralytics/ultralytics\">YOLO11</a> is the latest version of the YOLO (You Only Look Once) AI models developed by <a href=\"https://ultralytics.com\">Ultralytics</a>. We hope that the resources in this notebook will help you get the most out of YOLO11. Please browse the YOLO11 <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64B195mdjxQd"
      },
      "source": [
        "# Automatic Number Plate Recognition using Ultralytics YOLO11 + OpenAI `gpt-4o-mini`\n",
        "\n",
        "This notebook provides a comprehensive guide to implementing automatic number plate recognition (ANPR) using the YOLO11 model in combination with `gpt-4o-mini`.\n",
        "\n",
        "## What is Automatic Number Plate Recognition (ANPR)?\n",
        "Automatic number plate recognition is a technology designed to identify and extract vehicle number plate information from images or videos. By leveraging the powerful capabilities of Ultralytics YOLO11 for object detection and OpenAI `gpt-4o-mini` for text recognition, ANPR becomes an efficient solution for automating vehicle identification tasks.\n",
        "\n",
        "## Why Use YOLO11 + GPT-4o-Mini for ANPR?\n",
        "\n",
        "- Accurate Detection: YOLO11‚Äôs object detection capabilities ensure precise localization of license plates in various conditions, such as low light or high-speed movement.\n",
        "\n",
        "- Seamless Text Recognition: With `gpt-4o-mini`, extracted license plate regions are processed to recognize alphanumeric text accurately, even with variations in font, angle, or clarity.\n",
        "\n",
        "- Real-Time Processing: The integration allows for real-time vehicle monitoring, making it ideal for applications in traffic management, parking systems, and security surveillance.üöó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o68Sg1oOeZm2"
      },
      "source": [
        "### Setup\n",
        "\n",
        "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dSwz_uOReMI",
        "outputId": "db5a8c70-21c7-44c8-bd33-ff19fcb606fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.145 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 41.5/112.6 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "import base64\n",
        "\n",
        "import cv2\n",
        "import ultralytics\n",
        "from openai import OpenAI\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.downloads import safe_download\n",
        "from ultralytics.utils.plotting import Annotator, colors\n",
        "\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIZw8IJOk3Lb"
      },
      "source": [
        "### Download the Video and Model File\n",
        "\n",
        "Download the sample video we‚Äôll use for processing. If you prefer to use your own video file, downloading the sample is not necessary.  \n",
        "\n",
        "‚úÖ We‚Äôll download a license plate detection model `anpr-demo-model.pt` trained on a small dataset, designed to detect license plates in the sample video. You are welcome to use your own custom models as well.  \n",
        "\n",
        "‚ö†Ô∏è Note: This license plate detection model `anpr-demo-model.pt` is intended solely for proof of concept (POC) purposes and may only work with `anpr-demo-video.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkajxl6nlObU",
        "outputId": "56e910cc-e0ec-4304-c990-81c993792cb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('anpr-demo-model.pt')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# download the sample video file\n",
        "safe_download(\"https://github.com/ultralytics/assets/releases/download/v0.0.0/anpr-demo-video.mp4\")\n",
        "\n",
        "# download the sample model file\n",
        "safe_download(\"https://github.com/ultralytics/assets/releases/download/v0.0.0/anpr-demo-model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpKaC03TZDlr"
      },
      "source": [
        "### Read the Video and Model File\n",
        "\n",
        "You can either read the video file directly or stream content from an RTSP (Real-Time Streaming Protocol) source, offering flexible video input options to meet your requirements.\n",
        "\n",
        "‚úÖ By default, we will use the demo video `anpr-demo-video.mp4` downloaded in the previous step. Additionally, we‚Äôll set up the video writer to manage the output video processing.\n",
        "\n",
        "‚úÖ The model `anpr-demo-model.pt` will also be initialized in memory to handle the processing. You can also use your own model for license plate detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2daZuXgaZDFH"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(\"/content/anpr-demo-video.mp4\")\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "# Video writer\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "video_writer = cv2.VideoWriter(\"anpr-output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "# Load the Ultralytics YOLO license plate detection model\n",
        "model = YOLO(\"/content/anpr-demo-model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0QOXawB2-Zw"
      },
      "source": [
        "<img align=\"left\" src=\"https://github.com/user-attachments/assets/101152c3-25ba-48a7-9916-c32c3be5857e\" height=\"640\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7VkxQ2aeg7k"
      },
      "source": [
        "### Process Video Frames\n",
        "\n",
        "In this step, we will process video frames to detect objects, crop regions with padding, and extract license plate text using an OpenAI model. Here's how it works:\n",
        "\n",
        "‚úÖ Frames are read using OpenCV, and objects are detected using the YOLO11 model.\n",
        "Bounding boxes are adjusted with padding to crop regions of interest while ensuring proper boundaries.\n",
        "\n",
        "‚úÖ Cropped regions are encoded in base64 and sent to the `extract_text` function, which uses OpenAI‚Äôs model to retrieve license plate text.\n",
        "\n",
        "‚úÖ The extracted text is added as a label to the bounding box on the video frame.\n",
        "\n",
        "‚úÖ Processed frames are saved to an output video file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "Cx-u59HQdu2o",
        "outputId": "8ab04fd1-09e7-4e8f-eab2-2bae10f0be1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr==1.7.1\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (11.2.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (2.1.0)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr==1.7.1) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr==1.7.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr==1.7.1) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr==1.7.1) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr==1.7.1) (2025.5.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr==1.7.1) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr==1.7.1) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr==1.7.1) (3.0.2)\n",
            "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: easyocr\n",
            "  Attempting uninstall: easyocr\n",
            "    Found existing installation: easyocr 1.7.2\n",
            "    Uninstalling easyocr-1.7.2:\n",
            "      Successfully uninstalled easyocr-1.7.2\n",
            "Successfully installed easyocr-1.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "easyocr"
                ]
              },
              "id": "594cbf7b6e104a748ca706bf556d55a7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install EasyOCR\n",
        "!pip install easyocr==1.7.1\n",
        "\n",
        "# Import EasyOCR\n",
        "import easyocr\n",
        "\n",
        "# Assuming the necessary variables from previous cells are available:\n",
        "# cap = cv2.VideoCapture(...)\n",
        "# video_writer = cv2.VideoWriter(...)\n",
        "# model = YOLO(...)\n",
        "# ann = Annotator(...)\n",
        "# padding = 10\n",
        "\n",
        "# Initialize EasyOCR Reader\n",
        "reader = easyocr.Reader(['en']) # You can add more languages if needed\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, im0 = cap.read()\n",
        "\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    results = model.predict(im0)[0].boxes\n",
        "    boxes = results.xyxy.cpu()\n",
        "    clss = results.cls.cpu()\n",
        "\n",
        "    # Initialize Annotator inside the loop if im0 changes per frame, or outside if im0 is constant\n",
        "    # If im0 changes per frame and you want annotations on each frame:\n",
        "    ann = Annotator(im0, line_width=3)\n",
        "\n",
        "    for cls, box in zip(clss, boxes):\n",
        "        height, width, _ = im0.shape  # Get the dimensions of the original image\n",
        "\n",
        "        # Calculate padded coordinates\n",
        "        x1 = max(int(box[0]) - padding, 0)\n",
        "        y1 = max(int(box[1]) - padding, 0)\n",
        "        x2 = min(int(box[2]) + padding, width)\n",
        "        y2 = min(int(box[3]) + padding, height)\n",
        "\n",
        "        # Crop the object with padding\n",
        "        cropped_im = im0[y1:y2, x1:x2]\n",
        "\n",
        "        # Use EasyOCR to read text from the cropped image\n",
        "        results_easyocr = reader.readtext(cropped_im)\n",
        "\n",
        "        # Extract the recognized text\n",
        "        recognized_text = \"\"\n",
        "        for result in results_easyocr:\n",
        "            recognized_text += result[1] + \" \" # result[1] is the recognized text\n",
        "\n",
        "        print(f\"Extracted text: {recognized_text.strip()}\")\n",
        "\n",
        "        ann.box_label(box, label=str(recognized_text.strip()), color=colors(cls, True))  # Draw the bounding boxes\n",
        "\n",
        "    video_writer.write(im0)  # Write the processed video frame\n",
        "\n",
        "cap.release()  # Release the video capture\n",
        "video_writer.release()  # Release the video writer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X_dfSSH3ex5"
      },
      "source": [
        "<img align=\"left\" src=\"https://github.com/user-attachments/assets/d9f5efe7-d058-48f4-9515-f7ccf6084c2f\" height=\"640\">"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}